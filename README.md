# ğŸš€ ETL Workshop Project â€“ Candidate Selection

![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![MySQL](https://img.shields.io/badge/MySQL-8.0-orange.svg)

---

## ğŸ“Œ Description
This project implements an **end-to-end ETL pipeline** for a dataset of job candidates in the tech industry.  
The goal was to design a **Data Warehouse in MySQL** using a **star schema dimensional model**, and from it compute key **recruitment KPIs** and create **visualizations in Python (Matplotlib)**.

âš ï¸ All analysis and reports are generated directly from the **Data Warehouse**, not from the raw CSV file.

---

## ğŸ”„ Project Workflow
1. **Extract** â†’ Load raw candidate data from `candidates.csv` (~50k rows).  
2. **Transform** â†’  
   - Build clean dimension tables (`date`, `country`, `technology`, `seniority`, `candidate`).  
   - Define the field `hired`:  
     ```text
     hired = 1 if (code_challenge_score >= 7 AND interview_score >= 7), else 0
     ```  
3. **Load** â†’ Insert all dimensions and the fact table (`fact_selection`) into MySQL Workbench (`selection_dw`).  
4. **Analyze** â†’ Query the DW and generate recruitment KPIs. Results are exported to Python and visualized with Matplotlib.  

---

## ğŸ—„ï¸ Dimensional Model

Here is the star schema used in the project:

![Dimensional Model](figs/DDM.png)

**Fact Table**: `fact_selection`  
- Measures: `code_challenge_score`, `interview_score`, `hired (0/1)`  
- Foreign Keys: `date_id`, `candidate_id`, `country_id`, `technology_id`, `seniority_id`

**Dimension Tables**:  
- `dim_date (date_id, full_date, year, month)`  
- `dim_candidate (candidate_id, first_name, last_name, email, yoe)`  
- `dim_country (country_id, country)`  
- `dim_technology (technology_id, technology)`  
- `dim_seniority (seniority_id, seniority)`  

âœ¨ This **star schema** design makes queries fast and keeps the model simple for BI use cases.

---

## ğŸ“Š KPIs
A total of **six KPIs** were developed:

### âœ… Required KPIs
1. **Hires by Technology** â†’ Which technologies have the highest number of hires.  
2. **Hires by Year** â†’ Hiring trends over time.  
3. **Hires by Seniority** â†’ Which levels (Junior, Mid-level, Senior) are most often hired.  
4. **Hires by Country over Years** â†’ Focus on USA, Brazil, Colombia, and Ecuador.  

### â• Additional KPIs
5. **Hires by Experience Range (YOE buckets)** â†’ Distribution of hires by years of experience: 0â€“2, 3â€“5, 6â€“10, 10+.  
6. **Hire Rate** â†’ Recruitment efficiency:  
   \[
   HireRate = \frac{\text{Total Hired}}{\text{Total Applications}}
   \]

---

## ğŸ“ˆ Visualizations
The KPIs were visualized using **Matplotlib**, with direct SQL queries executed via **SQLAlchemy**.  

Examples:  
- ğŸ“Š Horizontal bar â†’ hires by technology  
- ğŸ“Š Vertical bar â†’ hires by year  
- ğŸ“ˆ Line chart â†’ hires by country  
- ğŸ“Š Bar chart â†’ hires by experience & seniority  
- ğŸ“‰ Single bar â†’ global hire rate  

All generated figures are stored in the `/figs` folder.  

### Hires by Technology
![Hires by Technology](figs/hires_por_tecnologia.png)

### Hires by Year
![Hires by Year](figs/hires_por_anio.png)

### Hires by Seniority
![Hires by Seniority](figs/hires_por_seniority.png)

### Hires by Country
![Hires by Country](figs/hires_por_pais_tiempo.png)

### Hires by Experience Range
![Hires by Experience Range](figs/hires_por_experiencia.png)

### Hire Rate
![Hire Rate](figs/hire_rate_global.png)

---

## âš™ï¸ Requirements
- Python 3.10+  
- Libraries: `pandas`, `sqlalchemy`, `pymysql`, `matplotlib`  
- MySQL Workbench 8.0 with the database `selection_dw`  

Install dependencies with:
```bash
pip install -r requirements.txt

## â–¶ï¸ How to Run
1. Clone the repository:
   git clone https://github.com/fabiaangzc/etl-workshop-candidate-selection.git
   cd etl-workshop-candidate-selection

2. Configure your MySQL credentials in db_conn.py

3. Run the ETL process:
   python etl.py

4. Generate KPIs and visualizations:
   python kpis.py

